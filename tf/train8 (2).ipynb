{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "train8.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3wF5wszaj97Y"
      },
      "source": [
        "# 以相似度为训练目标"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvbeBFq24MoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nnrWf3PCEzXL"
      },
      "source": [
        "## 训练数据生成器"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ez32DOYVjHFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/daibiaoxuwu/05wan9\n",
        "!git clone https://github.com/daibiaoxuwu/05wan9b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7aFIIQdwvPPM",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "datasizex = 15\n",
        "datasizey = 15\n",
        "picnum = 20\n",
        "def read_data(path1,path2):\n",
        "\n",
        "  x_train0 = []\n",
        "  y_train0 = dict()\n",
        "  idxes = dict()\n",
        "\n",
        "  #read clock\n",
        "  #read and /255\n",
        "  for filename in os.listdir(path2):\n",
        "      if 'git' in filename: continue\n",
        "      ans = cv2.imread(os.path.join(path2,filename), cv2.IMREAD_GRAYSCALE).astype('float32') /255.0      \n",
        "      ans = np.clip((ans-np.percentile(ans,10))/(np.percentile(ans,90)-np.percentile(ans,10)),0,1)\n",
        "      y_train0[int(filename[:-4])]=ans\n",
        "\n",
        "  keys = list(y_train0.keys())\n",
        "  \n",
        "  #read data\n",
        "  #add an axis and /255\n",
        "  \n",
        "  filelist = os.listdir(path1)\n",
        "  random.shuffle(filelist)\n",
        "  index = 0\n",
        "  for filename in filelist:\n",
        "      if 'git' in filename or 'c.jpg' in filename: continue\n",
        "      ftime = int(filename.split('_')[0])\n",
        "      value = max(filter(lambda t:t<ftime,keys))\n",
        "      if(abs(ftime-value)<10000):continue\n",
        "\n",
        "      x_train0.append(cv2.imread(os.path.join(path1,filename), cv2.IMREAD_GRAYSCALE).astype('float32')/255.0)\n",
        "      if value not in idxes:idxes[value]=[index]\n",
        "      else: idxes[value].append(index)\n",
        "      index += 1\n",
        "\n",
        "  #index data\n",
        "  print(len(x_train0),index,len(y_train0),len(idxes.keys()))\n",
        "  \n",
        "\n",
        "  #generator\n",
        "  def data_generator(ikeys_t):\n",
        "    while True:\n",
        "      dataout = []\n",
        "      for i in range(picnum):dataout.append([])\n",
        "      ansout = []\n",
        "      for train2 in range(100):\n",
        "        while True:\n",
        "          key = random.choice(ikeys_t)\n",
        "          idx2 = random.sample(idxes[key],picnum)\n",
        "          posx = random.randint(0,22-datasizex)\n",
        "          posy = random.randint(0,52-datasizey)\n",
        "          pic = x_train0[idx2[i]][posx:posx+datasizex,posy:posy+datasizey,np.newaxis]\n",
        "          if(np.sum(pic) < 98000):\n",
        "            for i in range(picnum):\n",
        "              dataout[i].append(x_train0[idx2[i]][posx:posx+datasizex,posy:posy+datasizey,np.newaxis])\n",
        "            ansout.append(y_train0[key][posx:posx+datasizex,posy:posy+datasizey])\n",
        "            break\n",
        "      yield [np.stack(data2) for data2 in dataout],np.stack(ansout)\n",
        "  def pic_generator(ikeys_t):\n",
        "    while True:\n",
        "      key = random.choice(ikeys_t)\n",
        "      idx2 = random.sample(idxes[key],picnum)\n",
        "      data=[x_train0[i][:,:,np.newaxis] for i in idx2]\n",
        "      ans=y_train0[key]\n",
        "      yield data,ans\n",
        "\n",
        "  #split\n",
        "  ikeys = list(idxes.keys())\n",
        "  random.shuffle(ikeys)\n",
        "  ltrain = int(len(ikeys)*0.8)  \n",
        "  train_generator = data_generator(ikeys[:ltrain])\n",
        "  test_generator = data_generator(ikeys[ltrain:])\n",
        "  train_pic_generator = pic_generator(ikeys[:ltrain])\n",
        "  test_pic_generator = pic_generator(ikeys[ltrain:])\n",
        "\n",
        "  return train_generator,test_generator,train_pic_generator,test_pic_generator\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfnEhjCTpreR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "train_generator,test_generator,train_pic_generator,test_pic_generator = read_data('05wan9','05wan9b')\n",
        "data,ans = next(train_generator)\n",
        "for pics in data:\n",
        "  for i in range(6):\n",
        "    plt.subplot(231+i)\n",
        "    plt.imshow(pics[i,:,:,0],cmap=cmap)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16T8NTH7IsZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(ans[0],cmap=cmap)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-lmz0keARk85"
      },
      "source": [
        "# 数据初始化完成。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VBl_86rWRk9F"
      },
      "source": [
        "# 装载Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0trJmd6DjqBZ",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# 安装 TensorFlow\n",
        "try:\n",
        "  # Colab only\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VxjclcvUlLv8"
      },
      "source": [
        "\n",
        "# 搭建模型和训练\n",
        "现在的网络去掉dropout则可以拟合10张图片，\n",
        "但是加上dropout就拟合不了。\n",
        "从7张拟合5张的，则可以在两个epoch（每个epoch抽取60000次）内到80%。\n",
        "\n",
        "继续训练20张，一个epoch就到60%。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nzds-gw75MO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = [tf.keras.layers.Input(shape=(datasizex,datasizey,1))  for i in range(picnum)]\n",
        "#inputs = tf.keras.layers.Input(shape=(5,12,12,1))\n",
        "\n",
        "conv_a1 = tf.keras.layers.Conv2D(32, (3, 3), padding = 'Same',  activation='relu')\n",
        "conva1 = [conv_a1(inputs[i]) for i in range(picnum)]\n",
        "conv_b1 = tf.keras.layers.Conv2D(32, (3, 3), padding = 'Same',  activation='relu')\n",
        "convb1 = [tf.keras.layers.Dropout(0.2)(tf.keras.layers.BatchNormalization()(conv_b1(conva1[i]))) for i in range(picnum)]\n",
        "\n",
        "conv_c1 = tf.keras.layers.Conv2D(32, (3, 3), padding = 'Same',  activation='relu')\n",
        "convc1 = [conv_c1(inputs[i]) for i in range(picnum)]\n",
        "conv_d1 = tf.keras.layers.Conv2D(32, (3, 3), padding = 'Same',  activation='relu')\n",
        "convd1 = [tf.keras.layers.Dropout(0.2)(tf.keras.layers.BatchNormalization()(conv_d1(convc1[i]))) for i in range(picnum)]\n",
        "\n",
        "convs1 = tf.stack(convb1)\n",
        "convf1 = [\n",
        "    tf.math.reduce_max(convs1, axis=[0]),\n",
        "    tf.math.reduce_mean(convs1, axis=[0]),\n",
        "    tf.math.reduce_min(convs1, axis=[0]),\n",
        "]\n",
        "\n",
        "conv_g1 = tf.keras.layers.Conv2D(32, (3, 3), padding = 'Same',  activation='relu')\n",
        "convg1 = [conv_g1(convf1[i]) for i in range(3)]\n",
        "\n",
        "convh1 = [tf.concat(convg1+[convd1[i]],axis=3) for i in range(picnum)]\n",
        "#------------------------------------------------------------------------------------------\n",
        "conv_a2 = tf.keras.layers.Conv2D(32, (3, 3), padding = 'Same',  activation='relu')\n",
        "conva2 = [conv_a2(convh1[i]) for i in range(picnum)]\n",
        "conv_b2 = tf.keras.layers.Conv2D(32, (3, 3), padding = 'Same',  activation='relu')\n",
        "convb2 = [tf.keras.layers.Dropout(0.2)(tf.keras.layers.BatchNormalization()(conv_b2(conva2[i]))) for i in range(picnum)]\n",
        "\n",
        "conv_c2 = tf.keras.layers.Conv2D(32, (3, 3), padding = 'Same',  activation='relu')\n",
        "convc2 = [conv_c2(inputs[i]) for i in range(picnum)]\n",
        "conv_d2 = tf.keras.layers.Conv2D(32, (3, 3), padding = 'Same',  activation='relu')\n",
        "convd2 = [tf.keras.layers.Dropout(0.2)(tf.keras.layers.BatchNormalization()(conv_d2(convc2[i]))) for i in range(picnum)]\n",
        "\n",
        "convs2 = tf.stack(convb2)\n",
        "convf2 = [\n",
        "    tf.math.reduce_max(convs2, axis=[0]),\n",
        "    tf.math.reduce_mean(convs2, axis=[0]),\n",
        "    tf.math.reduce_min(convs2, axis=[0]),\n",
        "]\n",
        "\n",
        "conv_g2 = tf.keras.layers.Conv2D(32, (3, 3), padding = 'Same',  activation='relu')\n",
        "convg2 = [conv_g2(convf2[i]) for i in range(3)]\n",
        "\n",
        "convh2 = [tf.concat(convg2+[convd2[i]],axis=3) for i in range(picnum)]\n",
        "\n",
        "#------------------------------------------------------------------------------------------\n",
        "conv_a3 = tf.keras.layers.Conv2D(32, (3, 3), padding = 'Same',  activation='relu')\n",
        "conva3 = [conv_a3(convh2[i]) for i in range(picnum)]\n",
        "conv_b3 = tf.keras.layers.Conv2D(32, (3, 3), padding = 'Same',  activation='relu')\n",
        "convb3 = [tf.keras.layers.Dropout(0.2)(tf.keras.layers.BatchNormalization()(conv_b3(conva3[i]))) for i in range(picnum)]\n",
        "\n",
        "conv_c3 = tf.keras.layers.Conv2D(32, (3, 3), padding = 'Same',  activation='relu')\n",
        "convc3 = [conv_c3(inputs[i]) for i in range(picnum)]\n",
        "conv_d3 = tf.keras.layers.Conv2D(32, (3, 3), padding = 'Same',  activation='relu')\n",
        "convd3 = [tf.keras.layers.Dropout(0.2)(tf.keras.layers.BatchNormalization()(conv_d3(convc3[i]))) for i in range(picnum)]\n",
        "\n",
        "convs3 = tf.stack(convb3)\n",
        "convf3 = [\n",
        "    tf.math.reduce_max(convs3, axis=[0]),\n",
        "    tf.math.reduce_mean(convs3, axis=[0]),\n",
        "    tf.math.reduce_min(convs3, axis=[0]),\n",
        "]\n",
        "\n",
        "conv_g3 = tf.keras.layers.Conv2D(32, (3, 3), padding = 'Same',  activation='relu')\n",
        "convg3 = [tf.keras.layers.Dropout(0.2)(tf.keras.layers.BatchNormalization()(conv_g3(convf3[i]))) for i in range(3)]\n",
        "\n",
        "convh3 = tf.concat(convg3+convd3,axis=3)\n",
        "\n",
        "convx1 = tf.keras.layers.Conv2D(32, (3, 3), padding = 'Same',  activation='relu')(convh3)\n",
        "convx3 = tf.keras.layers.Conv2D(32, (3, 3), padding = 'Same', activation='relu')(convx1)\n",
        "'''\n",
        "convx3 = tf.keras.layers.Flatten()(convx2)\n",
        "\n",
        "#keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)\n",
        "convx4 = tf.keras.layers.Dropout(0.2)(convx3)\n",
        "\n",
        "convy1 = tf.keras.layers.Dense(22*52)(convx4)\n",
        "#convy3 = tf.keras.layers.Dense(10,activation='softmax')(convy1)\n",
        "convy4 = tf.keras.layers.Reshape([22,52])(convy1)\n",
        "'''\n",
        "\n",
        "convx4 = tf.keras.layers.Conv2D(1, (1, 1), padding = 'Same',  activation='relu')(convx3)\n",
        "#from tfp.experimental.substrates.numpy.stats import percentile\n",
        "#out = tf.keras.layers.Lambda(\n",
        "#    lambda ans: np.clip((ans-percentile(ans,10))/(percentile(ans,90)-percentile(ans,10)),0,1),\n",
        "#    (datasizex, datasizey)\n",
        "#)\n",
        "out = tf.keras.layers.Lambda(\n",
        "    lambda ans: tf.where(ans > 0.5, 1, 0),\n",
        "    (datasizex, datasizey)\n",
        ")(convx4)\n",
        "\n",
        "model2 = tf.keras.Model(inputs=inputs, outputs=out)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aiXxR3CUgs3P",
        "colab": {}
      },
      "source": [
        "#tf.reduce_mean(tf.image.ssim(reconstructed, truth, 1.0))\n",
        "model.compile(optimizer='adam',#tf.keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
        "                #loss=lambda y_pred,y_true:1-tf.reduce_mean(tf.image.ssim_multiscale(tf.clip_by_value(y_pred,0,1), y_true, 255))#-tf.reduce_mean(tf.image.psnr(tf.clip_by_value(y_pred,0,1), y_true, 1.0))\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['mse']\n",
        "              )\n",
        "            \n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMByUd_CZrg8",
        "colab_type": "text"
      },
      "source": [
        "# 开始训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxfcLl_6DHaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r '/content/drive/My Drive/checkpoints/training_9/' '/content/drive/My Drive/checkpoints/training_10/'\n",
        "#!rm -r  '/content/drive/My Drive/checkpoints/training_7/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qeXrdtDT41r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "import math\n",
        "#reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=5, mode='auto')\n",
        "def step_decay(epoch):\n",
        "  initial_lrate = 0.001\n",
        "  drop = 0.5\n",
        "  epochs_drop = 10.0\n",
        "  lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "  return lrate\n",
        "  \n",
        "reduce_lr = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
        "'''\n",
        "#model.load_weights('/content/drive/My Drive/checkpoints/checkpoints/my_checkpoint')\n",
        "#os.mkdir('/content/drive/My Drive/checkpoints')\n",
        "checkpoint_path = \"/content/drive/My Drive/checkpoints/training_10/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "os.mkdir(checkpoint_dir)\n",
        "model.load_weights(\"/content/drive/My Drive/checkpoints/training_9/cp.ckpt\")\n",
        "# Create checkpoint callback\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
        "                                                 save_weights_only=False,\n",
        "                                                 verbose=1)\n",
        "\n",
        "\n",
        "model.fit_generator(train_generator,epochs=200,steps_per_epoch=2000,validation_data=test_generator,validation_steps=50,validation_freq=1,callbacks=[cp_callback])#,callbacks=[reduce_lr])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln87VWvG8-PF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#model.fit_generator(train_generator,epochs=200,steps_per_epoch=2000,validation_data=test_generator,validation_steps=50,validation_freq=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FDL6IYRQtz9",
        "colab_type": "text"
      },
      "source": [
        "# 测试正确率"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUmXvr4a9Vl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "    exp_x = np.exp(x)\n",
        "    softmax_x = exp_x / np.sum(exp_x)\n",
        "    return softmax_x \n",
        "\n",
        "data,ans = next(train_generator)\n",
        "data = [data[x][5][np.newaxis,:,:,:] for x in range(picnum)]\n",
        "ans = ans[5]\n",
        "import matplotlib\n",
        "cmap = matplotlib.cm.gray \n",
        "cmap.set_bad(color='black')\n",
        "\n",
        "pred = model2.predict(data)\n",
        "pred = np.clip(pred,0,1)\n",
        "pred = (pred-np.min(pred))/(np.max(pred)-np.min(pred))\n",
        "pred = softmax(pred)\n",
        "#pred[0,0,0,0]=0\n",
        "ans[0,0]=0\n",
        "for i in range(5):\n",
        "  plt.subplot(321+i)\n",
        "  plt.imshow(data[i][0,:,:,0],cmap=cmap)\n",
        "\n",
        "plt.show()\n",
        "plt.imshow(pred[0,:,:,0],cmap=cmap)\n",
        "plt.show()\n",
        "ans = np.clip((ans-np.percentile(ans,10))/(np.percentile(ans,90)-np.percentile(ans,10)),0,1)\n",
        "plt.imshow(ans,cmap=cmap)\n",
        "plt.show()\n",
        "from skimage.metrics import peak_signal_noise_ratio,structural_similarity, mean_squared_error\n",
        "print(peak_signal_noise_ratio(np.clip(pred[0,:,:,0],0,1),ans))\n",
        "print(structural_similarity(np.clip(pred[0,:,:,0],0,1),ans))\n",
        "print(mean_squared_error(np.clip(pred[0,:,:,0],0,1),ans))\n",
        "pic = np.ones(ans.shape)\n",
        "print(mean_squared_error(np.clip(pic*5-4,0,1),ans))\n",
        "#ssims = tf.image.ssim(tf.clip_by_value(pred[0],0,1), ans, 1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu61jCnJ0fQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_pic(xpics,model):\n",
        "  # xpic: array of 5 with size(22,52,1)\n",
        "  picout = np.zeros((22,52),dtype=np.float32)\n",
        "  piclayer = np.zeros((22,52),dtype=int)\n",
        "  for posx in range(0,23-datasizex):\n",
        "    for posy in range(0,53-datasizey):\n",
        "      dataout = [pic[np.newaxis,posx:posx+datasizex,posy:posy+datasizey] for pic in xpics]\n",
        "      pred = model.predict(dataout)*2-1\n",
        "      picout[posx:posx+datasizex,posy:posy+datasizey] += pred[0,:,:,0]\n",
        "      piclayer[posx:posx+datasizex,posy:posy+datasizey] += np.ones((datasizex,datasizey),dtype=int)\n",
        "  picout /= piclayer\n",
        "  return picout\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAhBlSEAhnFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data,ans = next(train_pic_generator)\n",
        "\n",
        "import matplotlib\n",
        "cmap = matplotlib.cm.gray \n",
        "cmap.set_bad(color='black')\n",
        "\n",
        "pred = predict_pic(data,model2)\n",
        "\n",
        "pred[0,0]=0\n",
        "ans[0,0]=0\n",
        "for i in range(6):\n",
        "  plt.subplot(231+i)\n",
        "  plt.imshow(data[i][:,:,0],cmap=cmap)\n",
        "\n",
        "plt.show()\n",
        "plt.imshow(pred,cmap=cmap)\n",
        "plt.show()\n",
        "plt.imshow(ans,cmap=cmap)\n",
        "plt.show()\n",
        "from skimage.metrics import peak_signal_noise_ratio,structural_similarity, mean_squared_error\n",
        "print(peak_signal_noise_ratio(np.clip(pred,0,1),ans))\n",
        "print(structural_similarity(np.clip(pred,0,1),ans))\n",
        "print(mean_squared_error(np.clip(pred,0,1),ans))\n",
        "pic = np.ones(ans.shape)\n",
        "print(mean_squared_error(np.clip(pic,0,1),ans))\n",
        "#ssims = tf.image.ssim(tf.clip_by_value(pred,0,1), ans, 1.0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj-vki4n35hR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "checkpoint_path = \"/content/drive/My Drive/checkpoints/training_10/cp.ckpt\"\n",
        "os.mkdir(\"/content/drive/My Drive/checkpoints/training_10\")\n",
        "model.save_weights(checkpoint_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RU6V8-C8GZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "model.save(\"/content/drive/My Drive/checkpoints/training_10/1.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjLm6FKx7Ayy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls sample_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU9VXIB54t6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git config --global user.email \"dujl16@mails.tsinghua.edu.cn\"\n",
        "!git config --global user.name \"daibiaoxuwu\"\n",
        "!git init\n",
        "!git add 1.*\n",
        "!git add checkpoint\n",
        "!git commit -m \"first commit\"\n",
        "!git remote add origin https://github.com/daibiaoxuwu/ckpt9.git\n",
        "!git push -u origin master"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwKUUWgI7VoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git remote set-url origin https://github.com/daibiaoxuwu/ckpt9.git\n",
        "!git push -u origin master"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZr6-1qh4LJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2=tf.keras.models.load_weights('1.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZeJMSce49b-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2.load_weights('1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bySeLn4Z-pHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skimage.metrics import mean_squared_error as mse\n",
        "from skimage.metrics import peak_signal_noise_ratio,structural_similarity, mean_squared_error\n",
        "\n",
        "bigdiff = 1\n",
        "bigdata = 0\n",
        "bigans = 0\n",
        "bigpred = 0\n",
        "for it in range(100):\n",
        "  data,ans = next(train_generator)\n",
        "  pred = model.predict(data)\n",
        "  for it2 in range(100):\n",
        "    data2 = [data[x][it2][np.newaxis,:,:,:] for x in range(5)]\n",
        "    ans2 = ans[it2]\n",
        "    \n",
        "    diff = structural_similarity(np.clip(pred[it2,:,:,0],0,1),ans2)\n",
        "    if(diff < bigdiff):\n",
        "      bigdiff=diff\n",
        "      bigdata=data2\n",
        "      bigans=ans2\n",
        "      bigpred=pred[it2,:,:,0]\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "  plt.subplot(321+i)\n",
        "  plt.imshow(bigdata[i][0,:,:,0],cmap='gray')\n",
        "plt.show()\n",
        "plt.imshow(bigpred,cmap='gray')\n",
        "plt.show()\n",
        "plt.imshow(bigans,cmap='gray')\n",
        "plt.show()\n",
        "print(bigdiff)\n",
        "print(mean_squared_error(np.clip(bigpred,0,1),bigans))\n",
        "\n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6fXbUXf7E9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skimage.metrics import mean_squared_error as mse\n",
        "from skimage.metrics import peak_signal_noise_ratio,structural_similarity\n",
        "\n",
        "bigdiff = 0\n",
        "bigdata = 0\n",
        "bigans = 0\n",
        "bigpred = 0\n",
        "diffs=[]\n",
        "for it in range(100):\n",
        "  data,ans = next(train_generator)\n",
        "  pred = model.predict(data)\n",
        "  diffs2 = []\n",
        "  for it2 in range(100):\n",
        "    data2 = [data[x][it2][np.newaxis,:,:,:] for x in range(5)]\n",
        "    ans2 = ans[it2]\n",
        "     #   print(ans2.dtype)\n",
        "    #print(pred.dtype#)\n",
        "#    print(ans2.max(),ans2.min())\n",
        "#    print(pred[it2,:,:,0].max(),pred[it2,:,:,0].min())\n",
        "    diff = structural_similarity(np.clip(pred[it2,:,:,0],0,1),ans2)\n",
        "    diffs2.append(diff)\n",
        "  #diffs.append(np.average(diffs2))\n",
        "  diffs+=diffs2\n",
        "plt.hist(diffs)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}